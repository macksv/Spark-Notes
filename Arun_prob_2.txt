Arun Prob 2

sqoop import --connect "jdbc:mysql://ms.itversity.com/retail_db" --username retail_user --password itversity --table products --target-dir "/user/macksv17/products" --as-textfile --fields-terminated-by "|"

hadoop fs -mv /user/macksv17/products /user/macksv17/problem2/


hadoop fs -chmod 765 /user/macksv17/problem2/products/*

 productRDD = sc.textFile("/user/macksv17/problem2/products")
 
>>> prodFiltered = productRDD.filter(lambda p:float(p.split("|")[4]) < 100)
>>> for i in prodFiltered.take(10): print(i)
...
1|2|Quest Q64 10 FT. x 10 FT. Slant Leg Instant U||59.98|http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy
3|2|Under Armour Men's Renegade D Mid Football Cl||89.99|http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat
4|2|Under Armour Men's Renegade D Mid Football Cl||89.99|http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Clea


**Convert RDD to DF for easier manipulation

 productDF = productRDD.map(lambda p: Row(product_id = int(p.split("|")[0]), product_category_id = int(p.split("|")[1]), \
...  product_name = p.split("|")[2], product_desc = p.split("|")[3], product_price = float(p.split("|")[4]), \
... product_image = p.split("|")[5])).toDF()


productDF.select("product_id", "product_category_id", "product_name", "product_price").filter(productDF["product_price"] < 100).show()


productSummary = sqlContext.sql("select product_category_id, max(product_price) as max_price, count(*) as tot_count, \ 
    avg(product_price) as avg_price, min(product_price) as min_price from products where product_price < 100 \
	group by product_category_id")

	
>>> sqlContext.setConf("spark.sql.avro.compression.codec", "snappy")
>>> productSummaryDF.coalesce(1).write.format("com.databricks.spark.avro").save("/user/macksv17/problem2/products/result-sql")




