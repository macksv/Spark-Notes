
sqoop import --connect jdbc:mysql://database.example.com/employees


--delete-target-dir
--table <table-name>
--target-dir <dir>
--warehouse-dir <dir>
--where <where clause>
--as-avrodatafile
--as-textfile
--as-parquetfile
-z,--compress
--compression-codec <c>


sqoop list-databases --connect jdbc:mysql://ms.itversity.com:3306 \
--username retail_user --password itversity

sqoop list-tables --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user --password itversity

sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user --password itversity \
--table orders \
--target-dir "/user/macksv17/mytest/orders_avro" \
--delete-target-dir \
--as-avrodatafile \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec

 
++++++++ to import to as orc, use DF and DF.write.orc
sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user --password itversity \
--table orders \
--target-dir "/user/macksv17/mytest/orders_text" \
--delete-target-dir \
--as-textfile

>>> df = sqlContext.read.text("/user/macksv17/mytest/orders_text")
>>> df.show()


++++++++ do a grep compress * in /etc/hadoop/

<value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec</value>





