
mysql -u retail_user -h ms.itversity.com -p

sqoop list-tables \
   --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
   --username retail_user --password itversity 
   
   
sqoop eval \
   --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
   --username retail_user \
   --password itversity \
   --query "select * from customers limit 10"

   
   
sqoop import \
   --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
   --username retail_user --password itversity \
   --table customers \
   --where "customer_state = 'CA'" \
   --fields-terminated-by '|' \
   --target-dir "/user/macksv17/problem1/customers/avrodata" \
   --compression-codec "org.apache.hadoop.io.compress.SnappyCodec" \
   --delete-target-dir \
   --as-avrodatafile
   
++++++++++++++++++++++++++++++++++++++++++++




sqoop import --connect "jdbc:mysql://ms.itversity.com:3306/retail_db" \
--username retail_user --password itversity \
--table customers \
--target-dir /user/macksv17/problem1/customers/text2 \
--fields-terminated-by '^' \
--columns "customer_id,customer_lname,customer_street"


sqoop export \
   --connect jdbc:mysql://ms.itversity.com:3306/retail_export \
   --username retail_user --password itversity \
   --table mv_customer_new \
   --fields-terminated-by '^' \
   --export-dir /user/macksv17/problem1/customers/text2 
   


>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Question 3 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<


sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
    --username retail_user --password itversity \
	--table orders \
	--target-dir "/user/macksv17/problem2/avro" \
	--delete-target-dir \
	--as-avrodatafile
	
	
++++ Use spark to read avro file into df	

	pyspark --packages com.databricks:spark-avro_2.10:2.0.1
	
	df = sqlContext.read.format("com.databricks.spark.avro").load("/user/macksv17/problem2/avro/part-m-00000.avro")
	
	sqlContext.setConf("spark.sql.parquet.compression.codec", "snappy")
	df.write.parquet("/user/macksv17/problem2/parquet-snappy")
	
	++ to test ++
	>>> sqlContext.read.parquet("/user/macksv17/problem2/parquet-snappy").show
<bound method DataFrame.show of DataFrame[order_id: int, order_date: string, order_customer_id: int, order_status: string]>
>>> sqlContext.read.parquet("/user/macksv17/problem2/parquet-snappy").first()
Row(order_id=11, order_date=u'2013-07-25 00:00:00.0', order_customer_id=11600, order_status=u'DUMMY')
>>>


>>>>>>>>>>>>>>> Question 4 <<<<<<<<<<<<<<


sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
    --username retail_user --password itversity \
	--table orders \
	--target-dir "/user/macksv17/practice4/question3/orders" \
	--delete-target-dir \
	--as-parquetfile
	
	

sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
    --username retail_user --password itversity \
	--table customers \
	--target-dir "/user/macksv17/practice4/question3/customers" \
	--delete-target-dir \
	--as-parquetfile


In pyspark

pyspark --packages com.databricks:spark-csv_2.10:1.5.0



orders_DF = sqlContext.read.load("/user/macksv17/practice4/question3/orders")
orders_DF.registerTempTable("orders_DF_table")
sqlContext.sql("Select * from orders_DF_table limit 5").show()

customers_DF = sqlContext.read.load("/user/macksv17/practice4/question3/customers")
customers_DF.registerTempTable("customers_DF_table")
sqlContext.sql("Select * from customers_DF_table limit 5").show()


sqlContext.sql("Select o.*, c.customer_lname from orders_DF_table o \
   join customers_DF_table c limit 5").show()



resultDF = sqlContext.sql("select c.customer_id, c.customer_fname, o.order_id, o.order_status from orders_DF_table o \
join customers_DF_table c on o.order_customer_id = c.customer_id \
where o.order_status like 'PENDING%'")


resultDF.write.save("/user/macksv17/practice4/question3/results_output", format="json")

** to save as csv
orders_DF.write.format("com.databricks.spark.csv").save("/user/macksv17/practice4/question3/results_output")

orders_DF.write.format("com.databricks.spark.csv").mode("Overwrite").save("/user/macksv17/practice4/question3/results_output")

orders_DF.write.format("com.databricks.spark.csv").mode("Overwrite").option("delimiter", "|").save("/user/macksv17/practice4/question3/results_output")

orders_DF.write.format("com.databricks.spark.csv").mode("Overwrite").option("delimiter", "|") \
.option("header", "true").save("/user/macksv17/practice4/question3/results_output")




================================


** the dates are in Unix timestamp format

orders_DF.write.format("com.databricks.spark.csv").mode("Overwrite").option("delimiter", "|") \
.option("header", "true").save("/user/macksv17/practice/files/orders_csv")


orders_DF.write.format("json").mode("Overwrite").option("delimiter", "|") \
.option("header", "true").save("/user/macksv17/practice/files/orders_json")


orders_DF.write.format("parquet").mode("Overwrite") \
.option("header", "true").save("/user/macksv17/practice/files/orders_parquet")


orders_DF.write.format("orc").mode("Overwrite") \
.option("header", "true").save("/user/macksv17/practice/files/orders_orc")


???? AVRO not working

sc.setLogLevel("ERROR")
orders_DF = sqlContext.read.load("/user/macksv17/practice4/question3/orders")

orders_DF.write.format("com.databricks.spark.avro").mode("Overwrite") \
.option("header", "true").save("/user/macksv17/practice/files/orders_avro")



>>>>>>>>>>>>>>>>>>>>>>> TO READ  <<<<<<<<<<<<<<<<<<<<<<<

















   